---
title: |
  | No bang for your buck?
  | On accuracy incentives in expectation aggregation
authors:
  - name:
    address:
abstract: |
  Due to the absence or cost of acquiring relevant historical data, many forecasting questions are answered by soliciting and aggregating the expectations of stakeholders. Prediction markets offer a particularly systematic way of doing just that, which moreover tends to yield accurate outputs. A common explanation for this is that such markets incentivise accuracy. However, the present paper reports on an experimental study involving participants being asked to perform two estimation tasks, mirroring the two main incentive structures on prediction markets, and finds minimal differences in accuracy on each task compared to a control condition. This calls into question the idea that prediction markets are successful on account of their incentive structure. Instead, it is argued that such markets are best understood as a special case of so-called expectation polls --- polls or surveys asking people about participants’ expectations (e.g., “Who will win the election?”) as opposed to about their intentions (e.g., “How would you vote, if there were an election today?”) --- and that one plausible explanation for the relative success of prediction markets is at least partly that they ask the right type of question: a predictive question, tapping into respondents’ expectations rather than their intentions or preferences.

acknowledgements:
  
keywords:
  - accuracy incentives
  - expectation aggregation
  - prediction markets
  - opinion polling
  
fontsize: 11pt
spacing: oneline # could also be oneline
#classoptions:
#  - endnotes
bibliography: refs.bib
output: rticles::oup_article
header-includes:
  - \usepackage[font={footnotesize}]{caption}
  - \usepackage{float}
#  - \usepackage[nomarkers,tablesfirst]{endfloat} # For figures and tables at end
#  - \usepackage{lineno} # For line numbering
#  - \linenumbers # For line numbering
---

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE) # By default, hide code; set to TRUE to see code
knitr::opts_chunk$set(fig.pos = 'H') # Places figures in text
knitr::opts_chunk$set(fig.align="center") # Centers figures
knitr::opts_chunk$set(out.width = '85%', dpi=300) # Figure resolution and size
knitr::opts_chunk$set(fig.env="figure") # Latex figure environment

library(dplyr)
library(broom)
library(tidyr)
library(ggplot2)
library(pscl)
library(kableExtra)
library(ggpubr)
library(stargazer)
library(texreg)

all_data <- read.csv("../data/all_data.csv")
```

# Introduction

Will some new piece of legislation negatively effect a specific, commercial sector? Will the COVID-19 pandemic significantly alter how companies manage offices? Will a new form of technology substantially change consumer behavior? Due to an absence of historical data, or the cost of acquiring, processing, and maintaining such data, questions such as these are often answered by aggregating the judgments or expectations of different stakeholders [@hyndman2018]. The aggregation methods used in practice range from the less systematic (e.g., informal votes in a board room, or the ad hoc consultation of subject matter experts) to the more systematic. Prediction markets --- markets for placing bets on future or otherwise unknown events --- is one systematic method for expectation aggregation that moreover has been shown to offer a particularly efficient way of generating accurate estimates in a wide range of areas [@hahntetlock2006], including politics [@berg_rietz_2014; @bergetal2008; @FORSYTHE199983], sports [@luckneretal2008; @deschampsetal2007; @debnathetal2003], business [@oleary2011; @chenplott2002; @spannetal2003], medicine and health care [@polgreen2007; @mattingly2014], and entertainment [@McKenzie2013; @Pennock2001].

Given their success, we will want to know why they tend to generate accurate outputs. A common explanation is that prediction markets _incentivise accuracy_. As @sunstein2006-hahntetlock notes, "a correct answer is rewarded and an incorrect one punished. Hence, investors have a strong incentive to be right" (88). Specifically, as suggested by @hall2010, there is "a powerful financial incentive for truthful revelation --- a profit motive --- that serves as a countervailing force to emotional, political, and professional motivations" (32). However, as suggested by Kenneth Arrow and others, it is also thought that "the potential for profit (and loss) creates strong incentives to search for better information" [@arrow2008: 877]. Indeed, the idea that incentives on prediction markets perform this dual role of truthful revelation and information discovery is very common in the literature. Three representative statements are as follows:

> The price mechanism rewards participants for making accurate predictions (i.e., they win money) and punishes them otherwise (i.e., they lose money). Participants thus have an incentive to actively look for information and immediately and truthfully reveal it to the market [@graefe2017: 38].

> [...] because people stand to gain or lose from their investments, they have a strong incentive to use (and in that sense to disclose) whatever private information they hold; they can capture, rather than give to others, the benefits of disclosure. [...] Prediction markets also impose strong incentives for traders to ferret out accurate information [@sunstein2006: 205-6].

> [...] the markets provide an incentive to generate, gather and process information across information sources and in a variety of ways. Traders who perform these tasks well, prosper. Those who don't may go broke, may drop out of the market, and appear less likely to set forecast determining prices. [@bergetal2008: 286].

There are, however, a number of reasons to be skeptical of this explanation of prediction market accuracy. For one thing, as for _financial_ incentives specifically, the accuracy difference between play- and real-money markets turns out to be either non-existent [@ServanSchreiber2004] or small and context dependent [@Mchugh2012]. For another, there is reason to believe that accuracy incentives of _any_ kind should not make a difference to accuracy, at least in interesting cases involving non-trivial judgment tasks. As noted by @Camerer1999 in a review of the effects of financial incentives on performance in experimental settings, "incentives sometimes improve performance, but often don't" (34). Specifically, incentives are helpful primarily on clerical, and other clearly effort-responsive tasks. That is, if doing well simply means trying harder, then incentives can make a difference. By contrast, when doing well requires knowing more or having more skill, incentives do not help. Trying harder --- whether to report your beliefs truthfully, or to collect further information --- is of little use if you do not know what you are doing.

It is important to note that none of this impugns the practice of using incentives to get participants to engage with the relevant tasks to begin with. To avoid confusion, we need to distinguish between incentivisation for engagement and for accuracy. When _everyone_ is rewarded (whether financially or not) for performing some particular task, we are incentivising for engagement; when participants (additionally) are rewarded _differently_ depending on their level of performance (the better you perform, the more you get), we are incentivising for (in this case) accuracy. Throughout, when talking about incentivisation, it is the latter that is being referred to.

We have some reason, then, to suspect that accuracy incentives make no difference on, and as such do not explain the accuracy of, the type of expectation aggregation taking place on prediction markets. In order to determine whether this suspicion is borne out by the evidence, the present paper reports on an experimental study involving participants being asked to perform two estimation tasks. Specifically, to mirror the type of incentive structure that exists on prediction markets --- so called external resolution versus self-resolution [@ahlstromvij_jpm_2019] --- participants in two treatment conditions were rewarded either with reference to successfully estimating some external fact or event (a form of external resolution), or with reference to successfully predicting the mean response of participants about these external facts and events (a form of self-resolution, since the question is "resolved" with reference to only internal factors). The results indicate significantly higher effort on the part of participants in the incentivised treatment conditions, as measured by completion time, yet a minimal differences in accuracy between each of the two treatment conditions and a control condition, where participants did not receive any accuracy incentive. This calls into question the idea that prediction markets are successful on account of their incentive structure. 

In the discussion section, it is suggested that, in light of these results, prediction markets are best understood as a special case of what is sometimes referred to as _expectation polls_: polls or surveys asking people about participants' expectations in general (e.g., "Who will win the election?") [@murr2021; @RothschildWolfers2011] or in relation to their social circles specifically [@ahlstromvij-socialcircle; @Galesic:2018aa] as, opposed to about participants' intentions (e.g., "How would you vote, if there were an election today?"). Such expectation polls tend to outperform traditional intention aggregation polls, likely because the former tap into people's non-trivial amounts of knowledge about the intentions and preferences of others, who are thereby implicitly sampled as well [@ahlstromvij-socialcircle]. Given this, it is argued that one plausible explanation for the accuracy of prediction markets, especially compared to traditional surveys, is therefore at least partly that they ask the right type of question: a predictive question about respondents' expectations rather than their own intentions or preferences.

# Method and sample

In order to evaluate whether accuracy incentives matter for the accuracy of prediction markets, two estimation tasks were developed as part of an experimental design. The first task asked participants to estimate the probability of drawing a black ball out of a (virtual) urn of black and white balls, in light of a sample of 10 balls that had already been drawn, showing 7 out of 10 being black. In the second task, participants were asked to estimate what proportion of the UK's population would come to have received a first shot of a COVID-19 vaccine on February 1, 2021, in light of official figures estimating it to be 4.9% on December 15, 2020.

## Recruitment

Participants were recruited via the online recruitment platform Prolific (www.prolific.co).^[Ethical approval was obtained from the College Ethics Committee at [removed for blind review], prior to recruitment, with full details available on request.] Existing studies suggest that Prolific offers greater diversity among participants, and a smaller proportion of "professional survey takers" than other established platforms like Amazon Turk [@peer2022; @PEER2017153], as well as more control from the point of view of the researcher in terms of reliable pre-screening [@PALAN201822]. In this case, participants were pre-screened to be residing in the UK, and to have an approval rating on the platform of at least 99%. The recruitment commenced on January 24, 2021, with 30 participants in a "soft-launch" to ensure that there were no issues with the route between the recruitment platform and the survey experiment, and concluded the next day, on January 25, with a total of 1224 responses.

## Experimental conditions and incentives
Participants were randomly allocated to one of three conditions.^[See Section \ref{recruitment-texts} in the Appendix for the texts used to recruit participants to the different conditions.] In the _external resolution condition_, participants saw the following prompt as they were answering the first estimation question, about the the probability of drawing a black ball out of a (virtual) urn:

> _PLEASE NOTE: The "correct" answer will be determined by the actual proportion of black balls in the (virtual urn). For example, if there are 70% black balls, the correct answer is 70%. Everyone giving the correct answer will take part in a lottery, where one person will be awarded £10, on top of the incentive received for participating._

The same prompt appeared again, _mutatis mutandis_, as participants were answering the second estimation question, regarding what proportion of the UK's population would be reported by the government on February 1, 2021 as having received (at least) one shot of a COVID-19 vaccine.

In the _self-resolution condition_, participants saw the following prompt as they were answering the first estimation question:

> _PLEASE NOTE: The "correct" answer will be determined by the mean response given by participants. For example, if participants answer 1%, 2% and 3%, the "correct" answer is 2%. Everyone giving the correct answer will take part in a lottery, where one person will be awarded £10, on top of the incentive received for participating._

The same prompt appeared, _mutatis mutandis_, as participants were answering the second estimation question.

In the _control condition_, participants were not told anything about being paid beyond the incentive received for participating. 

## Incentives
The base incentive received for participating across all conditions was £0.38, corresponding to an estimated £7.60 per hour, given that taking part in the study was expected to take about 3 minutes. The idea behind making the potential reward of £20 (£10 per question) in the treatment conditions substantially larger than the base incentive --- 52 times higher, to be exact --- was exactly to incentivise the type of truthful revelation and information discovery that, as we have seen, are taken by many to explain the accuracy of prediction markets.

```{r time_spent, echo=FALSE, message=FALSE}
time_control <- all_data$time_taken[all_data$condition=="no_res"]
time_self_res <- all_data$time_taken[all_data$condition=="self_res"]
time_external <- all_data$time_taken[all_data$condition=="external"]

wilcox_external <- wilcox.test(time_control,time_external)
wilcox_selfres <- wilcox.test(time_control,time_self_res)
```

Respondents, moreover, seem to have responded to the incentive in the way we would expect if they were trying harder, or at least spending more time on the task. In the control condition, the median completion time was 2 minutes and 7 seconds. By contrast, in the self-resolution condition the median completion time was significantly higher, at 3 minutes and 1 second (Wilcoxon rank sum test; _W_ = `r format(wilcox_selfres$statistic[[1]], scientific=FALSE)`, _p_ < 0.0001). In the external resolution condition, the median completion time was also significantly higher than in the control, at 3 minutes and 25 seconds (Wilcoxon rank sum test; _W_ = `r format(wilcox_external$statistic[[1]], scientific=FALSE)`, _p_ = < 0.0001).^[The time spent on the survey was unavailable for 9 respondents, as they had not input their Prolific ID correctly, and could as such not be matched up with the relevant entries in the completion time data.] In light of this, it seems clear that participants invested greater effort by spending more time on the survey --- and in so doing, potentially also seeking out further information (at least in the vaccination task) --- in the two incentivised conditions.^[Might the greater amount of time spent in the two treatment conditions simply reflect participants spending (more) time making sense of the instructions? If that were so, we would expect that those who spent more time exhibited greater comprehension, as captured by the manipulation check (see next section). However, while there was a 5 second difference between the median time of the two groups  (3 min. 12 sec. for those failing the comprehension check, and 3 min. 17 sec. for those passing it), the distributions of time spent were very similar, and the difference not statistically significant (Wilcoxon rank sum test; _W_ = 34772, _p_ = 0.7125).]

## Disqualifications and manipulation check

19 of the 1224 responses were from respondents who had managed to participate more than once. These responses were removed prior to analysis, alongside 9 responses with missing estimates, leaving 1196 responses.

In order to evaluate the effect of accuracy incentives, it was important that those taking part understood the relevant incentivisation structure. For that reason, a manipulation check was included at the very end of the study, as follows (with 1 and 2 randomized):

> _On the previous pages, we told you how we would determine the “correct” answers to the questions. How will these be determined?_
  
>_1. By the actual proportion of black balls in the urn for the first question, and by the numbers reported on the official UK Government website for data on Coronavirus on February 1 for the second question._  

>_2. By the mean answer given by participants in this study for each of the two questions._

The manipulation check disqualified 7 participants in the external resolution condition, and 91 in the self-resolution condition, leaving a final sample of 1098 participants, distributed across measured demographics as in Table \ref{tab:participant_table} in the Appendix. The table shows a fairly balanced demographic distribution across the conditions, which suggests that random allocation in recruitment was largely successful, and not affected by disqualification.

# Hypotheses

Two hypotheses were formulated. First, given the results discussed in Section \ref{introduction} about incentives primarily making a difference in clerical and as such effort-responsive tasks, the following was expected:

> LOWER ERROR. For each of the two incentivised conditions (i.e., external resolution and self-resolution), the average error will be significantly lower in the urn estimation task, compared to the control condition.

Error was measured in terms of the average absolute percentage point deviation from the correct value. For example, if a participant estimated the proportion to be 30% and the correct answer was 10%, the error for that participant would be 20 percentage points, and the error for the condition as a whole the average error for all participants in that condition. For the urn task, the correct value was stipulated as 70% in the external resolution condition, the idea being that, having nothing else to go on, participants would judge that the small sample of 7 black balls out of 10 reflected the distribution in the urn from which those samples had been drawn. In the self-resolution condition, the "correct" estimate was identified with the mean estimate, as per the instructions given to participants. The mean estimate ended up being 64%.

However, since we are ultimately investigating whether accuracy incentives make a difference, we also need to look beyond classical null hypothesis testing, and consider practically relevant effect sizes. After all, with a large enough sample, any effect will come out significant, whether substantial enough to be practically relevant. For that reason, we need to settle on an effect size that is practically relevant and consider whether any effect is significant using equivalency testing [@lakens2018]. What is a big enough difference for practical purposes in our case? There is no established standard for the simple reason that what level of error is tolerable is highly context dependent. The same would, therefore, need to go for any _difference_ in error between competing method, which is what we are concerned with here. In the absence of a received standard, it was decided that a +/- 5 percentage point difference would be deemed to fall within the bounds of the practically equivalent. It is ultimately up to the reader to decide whether they deem these equivalency bounds appropriate for their purposes. 

Hence, the second hypothesis:

> PRACTICAL EQUIVALENCY. For each of the two incentivised conditions (i.e., external resolution and self-resolution), the average error will fall within +/- 5 percentage points of, and as such be practically equivalent to, the average error in the control condition.

For the vaccination estimation task, the correct estimate in the external resolution condition was given by the number of people in the UK who on February 1, 2021, was reported as having received (at least) one shot of a COVID-19 vaccine, divided by the most up to date estimate of the UK population size, and rounded to the nearest whole number. On February 1, this corresponded to 9,296,367 / 66,796,807 = 13.92%, for a rounded value of 14%.^[Vaccination figures were retrieved from the official UK Government figures at https://coronavirus.data.gov.uk/details/vaccinations, and UK population estimate from the UK Office for National Statistics at https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates, both on February 1, 2021.] In the self-resolution condition, the "correct" estimate was given by the mean estimate, which ended up being 15%.

# Results

The mean estimate and error by condition and estimation task is given in Table \ref{tab:est_errors}, and the distribution of estimates in Fig. \ref{fig:est_dist}. 

```{r est_errors, echo=FALSE, message=FALSE}
# table with mean estimates and errors
df_errors <- data.frame(tapply(all_data$urn, all_data$condition, mean),
                        tapply(all_data$urn, all_data$condition, sd),
                        tapply(all_data$urn_error, all_data$condition, mean),
                        tapply(all_data$urn_error, all_data$condition, sd),
                        tapply(all_data$vaccination, all_data$condition, mean),
                        tapply(all_data$vaccination, all_data$condition, sd),
                        tapply(all_data$vacc_error, all_data$condition, mean),
                        tapply(all_data$vacc_error, all_data$condition, sd))
df_errors <- t(df_errors)

colnames(df_errors) <- c("External resolution","Control","Self-resolution")
df_errors <- df_errors[,c(1,3,2)]
df_errors <- round(df_errors,2)

df_errors_1 <- df_errors[c(1,3,5,7),]
df_errors_2 <- df_errors[c(2,4,6,8),]

two_tables_into_one <- as.data.frame(do.call(cbind, lapply(1:ncol(df_errors_1), function(i) paste0(df_errors_1[ , i], " (", df_errors_2[ , i], ")"  ) )))
names(two_tables_into_one) <- names(df_errors_1)

row.names(two_tables_into_one) <- c("Urn: Mean estimate",
                                    "Urn: Mean error",
                                    "Vaccination: Mean estimate",
                                    "Vaccination: Mean error")

knitr::kable(two_tables_into_one,caption="Mean estimates (percent) and errors (percentage points) by condition (SD in paranthesis)",format="latex",
             booktabs=TRUE, escape = FALSE, longtable = TRUE, linesep = "", label="est_errors", 
             col.names = linebreak(c("External resolution",
                                     "Self-resolution",
                                     "Control"), align = "c")) %>%
  kable_styling(latex_options = c("hold_position", "repeat_header"))
    
```

```{r est_dist, fig.cap="Distribution of estimates", echo=FALSE, message=FALSE}
urn_dist <- all_data %>% 
  mutate(condition = dplyr::recode(condition,
                            "external" = "Ext. res.",
                            "no_res" = "Control",
                            "self_res" = "Self-res.")) %>% 
  ggplot() +
  aes(x = condition, y = urn, fill = condition) +
  geom_violin() +
  geom_jitter(alpha = 0.2) +
  xlab("Condition") +
  ylab("Estimate (%)") +
  theme_minimal() +
  theme(legend.position = "none")

vacc_dist <- all_data %>% 
  mutate(condition = dplyr::recode(condition,
                            "external" = "Ext. res.",
                            "no_res" = "Control",
                            "self_res" = "Self-res.")) %>% 
  ggplot() +
  aes(x = condition, y = vaccination, fill = condition) +
  geom_violin() +
  geom_jitter(alpha = 0.2) +
  xlab("Condition") +
  ylab("Estimate (%)") +
  theme_minimal() +
  theme(legend.position = "none")

ggarrange(urn_dist, vacc_dist, ncol = 2, nrow = 1, common.legend = FALSE, legend = NULL)
```

```{r princ_of_indiff, fig.cap="Distribution of estimates", echo=FALSE, message=FALSE}
princ_indiff <- all_data %>%
  mutate(princ_of_diff = ifelse(urn == 50, 1, 0)) %>%
  group_by(condition) %>% 
  count(princ_of_diff) %>% 
  mutate(prop = n/sum(n)) %>% 
  filter(princ_of_diff == 1)
```

Starting with the distribution of estimates in Fig. \ref{fig:est_dist}, the first thing to note is similarity across the three conditions. The second thing to note is that, unlike the vaccination estimation task, the distribution for the urn estimation task has not one but two clusters of estimates: one around 70%, and a smaller one around 50%. It's clear that at least some users approached the estimate task against the background of a principle of indifference, likely considering that the small sample they had access to didn't settle the question of the correct proportion, which therefore should be assumed to be 50%. This was the case for a minority of respondents, and moreover a similarly sized minority across the conditions (`r round(princ_indiff$prop[1]*100,0)`% for the external resolution condition; `r round(princ_indiff$prop[3]*100,0)`% for the self-resolution condition; and `r round(princ_indiff$prop[2]*100,0)`% for the control condition). Given this parity, the presence of such minority clusters do not affect any of our subsequent analysis about the comparative accuracy between conditions.

Turning to the errors, we see that the mean error is lower in the external resolution condition than in the control condition for the urn estimation task, and the error in the self-resolution condition is higher than in the control. The latter fact already tells us that LOWER ERROR --- the hypothesis that, for _each_ of the two incentivised conditions, the average error will be significantly lower in the urn estimation task, compared to the control --- is not borne out by the evidence. For this reason, we reject LOWER ERROR.

```{r toster, echo=FALSE, message=FALSE, results=FALSE, fig.show='hide'}
library(TOSTER)
bound <- 5
alpha <- 0.025
tost1_urn <- TOSTtwo.raw(m1=mean(all_data$urn_error[all_data$condition=="external"]), 
                     m2=mean(all_data$urn_error[all_data$condition=="no_res"]), 
                     sd1=sd(all_data$urn_error[all_data$condition=="external"]), 
                     sd2=sd(all_data$urn_error[all_data$condition=="no_res"]), 
                     n1=length(all_data$urn_error[all_data$condition=="external"]), 
                     n2=length(all_data$urn_error[all_data$condition=="no_res"]), 
                     low_eqbound=-bound, high_eqbound=bound, alpha = alpha,var.equal = FALSE)

tost2_urn <- TOSTtwo.raw(m1=mean(all_data$urn_error[all_data$condition=="self_res"]), 
                         m2=mean(all_data$urn_error[all_data$condition=="no_res"]), 
                         sd1=sd(all_data$urn_error[all_data$condition=="self_res"]), 
                         sd2=sd(all_data$urn_error[all_data$condition=="no_res"]), 
                         n1=length(all_data$urn_error[all_data$condition=="self_res"]), 
                         n2=length(all_data$urn_error[all_data$condition=="no_res"]), 
                         low_eqbound=-bound, high_eqbound=bound, alpha = alpha,var.equal = FALSE)

tost1_vacc <- TOSTtwo.raw(m1=mean(all_data$vacc_error[all_data$condition=="external"]), 
                      m2=mean(all_data$vacc_error[all_data$condition=="no_res"]), 
                      sd1=sd(all_data$vacc_error[all_data$condition=="external"]), 
                      sd2=sd(all_data$vacc_error[all_data$condition=="no_res"]), 
                      n1=length(all_data$vacc_error[all_data$condition=="external"]), 
                      n2=length(all_data$vacc_error[all_data$condition=="no_res"]), 
                      low_eqbound=-bound, high_eqbound=bound, alpha = alpha,var.equal = FALSE)

tost2_vacc <- TOSTtwo.raw(m1=mean(all_data$vacc_error[all_data$condition=="self_res"]), 
                      m2=mean(all_data$vacc_error[all_data$condition=="no_res"]), 
                      sd1=sd(all_data$vacc_error[all_data$condition=="self_res"]), 
                      sd2=sd(all_data$vacc_error[all_data$condition=="no_res"]), 
                      n1=length(all_data$vacc_error[all_data$condition=="self_res"]), 
                      n2=length(all_data$vacc_error[all_data$condition=="no_res"]), 
                      low_eqbound=-bound, high_eqbound=bound, alpha = alpha,var.equal = FALSE)

p_urn <- c(tost1_urn$TOST_p1,tost1_urn$TOST_p2,
           tost2_urn$TOST_p1,tost2_urn$TOST_p2)
adj_ps_urn <- p.adjust(p_urn, method="holm")
round(p_urn,5)
round(adj_ps_urn,5)

p_vacc <- c(tost1_vacc$TOST_p1,tost1_vacc$TOST_p2,
            tost2_vacc$TOST_p1,tost2_vacc$TOST_p2)
adj_ps_vacc <- p.adjust(p_vacc, method="holm")
round(p_vacc,5)
round(adj_ps_vacc,5)
```

However, we noted above that classical null hypothesis testing will not answer the question we are ultimately interested in, namely whether accuracy incentives make a _difference_. Specifically, we want to know whether PRACTICAL EQUIVALENCY holds --- the hypothesis that, for each of the two incentivised conditions, the average error will fall within +/- 5 percentage points of, and as such be practically equivalent to, the average error in the control condition. For purposes of testing this hypothesis, the R package `TOSTER` [@lakens2017; @r_core_team] was used to perform two one-sided t-tests in evaluating whether an effect size is large enough to be considered worthwhile.^[As generating one confidence interval in this case requires performing two one-sided t-tests, an alpha level of 0.025 was used, to give a 95% confidence interval. To provide further protection against inflated Type I errors from performing several t-tests, the p-values for the euivalency tests were also adjusted using the Holm method. Adjusted p-values were < 0.0001 and < 0.0001 for the external condition in the urn task and < 0.0001 and < 0.0001 for the vaccination task; and < 0.0001 and 0.0014 for the self-resolving condition in the urn task and < 0.0001 and < 0.0001 in the vaccination task.] 

```{r equiv_test, fig.cap="Equivalency tests on difference in mean error with a bound of +/-5 percentage points", echo=FALSE, message=FALSE, results=FALSE}
plot_tosts <- data.frame("task" = c("urn","urn","vaccine","vaccine"),
                         "condition" = c("Ext.","Self-res.","Ext.","Self-res."),
                         "difference" = c(tost1_urn$diff,
                                          tost2_urn$diff,
                                          tost1_vacc$diff,
                                          tost2_vacc$diff),
                         "upr" = c(tost1_urn$UL_CI_TOST,
                                   tost2_urn$UL_CI_TOST,
                                   tost1_vacc$UL_CI_TOST,
                                   tost2_vacc$UL_CI_TOST),
                         "lwr" = c(tost1_urn$LL_CI_TOST,
                                   tost2_urn$LL_CI_TOST,
                                   tost1_vacc$LL_CI_TOST,
                                   tost2_vacc$LL_CI_TOST))

tosterplot_1 <- ggplot(plot_tosts[1:2,], aes(x=condition, y=difference, fill=condition)) + 
  geom_pointrange(aes(ymin=lwr, ymax=upr), color="black", shape=21) +
  geom_hline(yintercept=0, linetype="11", color = "grey") +
  geom_hline(yintercept=-bound, linetype="11", color = "black") +
  geom_hline(yintercept=bound, linetype="11", color = "black") +
  xlab("Condition") +
  ylab("") +
  ggtitle("Urn") +
  #labs(subtitle = "Equivalency tests with equivalency bounds of +/-3 percentage points") +
  theme(plot.title = element_text(face = "bold")) +
  coord_flip() +
  theme_minimal() +
  theme(legend.position = "none") +
  theme(panel.grid.major = element_line(linetype = "dashed")) +
  theme(panel.grid.minor = element_line(linetype = "dashed"))

tosterplot_2 <- ggplot(plot_tosts[3:4,], aes(x=condition, y=difference, fill=condition)) + 
  geom_pointrange(aes(ymin=lwr, ymax=upr), color="black", shape=21) + # green
  geom_hline(yintercept=0, linetype="11", color = "grey") +
  geom_hline(yintercept=-bound, linetype="11", color = "black") +
  geom_hline(yintercept=bound, linetype="11", color = "black") +
  xlab("") +
  ylab("") +
  ggtitle("Vaccination") +
  #labs(subtitle = "Equivalency tests with equivalency bounds of +/-3 percentage points") +
  theme(plot.title = element_text(face = "bold")) +
  coord_flip() +
  theme_minimal() +
  theme(legend.position = "none") +
  theme(panel.grid.major = element_line(linetype = "dashed")) +
  theme(panel.grid.minor = element_line(linetype = "dashed"))

library(ggpubr)
ggarrange(tosterplot_1, tosterplot_2, ncol = 2, nrow = 1, common.legend = FALSE, legend = NULL)
```

If we look at any lack of overlap between the confidence intervals and the dashed vertical line at 0 in Fig. \ref{fig:equiv_test}, we see the outcome of a set of traditional null hypothesis tests. Specifically, we see that the only significant difference is between the self-resolving condition and the control for the urn task. (This was the aforementioned effect that was in the "wrong" direction, as far as our LOWER ERROR hypothesis was concerned.) The remaining differences are not significant. However, as we are performing an equivalency test, we want to consider whether the confidence intervals clear the outer equivalency bounds at +/- 5 percentage points. In so far as they do, the mean errors for the relevant condition and the control are practically equivalent. Indeed, we see that, in the vaccination condition, which is arguably the most realistic estimation task of the two, the mean errors are practically equivalent all the way down to 2.5 percentage points. Consequently, PRACTICAL EQUIVALENCY is consistent with the evidence.

# A third role for accuracy inventives?

In Section \ref{introduction}, we noted that the literature generally points to incentives on prediction markets performing the dual role of encouraging both truthful revelation and information discovery. The evidence provided for PRACTICAL EQUIVALENCY in the previous section calls into question whether incentives really offer such a mechanism --- if they did, then we would have expected to see a significant accuracy effect from the two incentivisation conditions, which we did not (indeed, on one case, we saw a significant _inaccuracy_ effect).

However, there is another manner in which incentives might be able to increase accuracy on prediction markets, and that is by drawing in knowledgeable people, motivated by the incentives, who might not have joined the markets in the absence of such incentives. One way to spell this out is with reference to work suggesting that a typical prediction market works by having a fairly large number of uninformed traders provide liquidity, while a small number of marginal traders opportunistically take positions opposite to those of the uninformed, and in virtue of their trading volume also end up driving the market [@FORSYTHE199983; see also @OlivenRietz2004]. These marginal traders tend to trade higher-than-average sums and to be active on the market on a higher-than-average number of days. They also earn higher-than-average returns, and are less prone to cognitive biases (such as assuming that their views are shared by others, or being inappropriately optimistic when interpreting evidence that supports their preferred outcomes).

Tying this back to accuracy incentives, the idea would be that incentives attract exactly such marginal traders, who in turn will drive accuracy. Notice, however, that suggesting that marginal traders _in actual fact_ drive accuracy on incentivised markets does not go to show that they, counterfactually, would _not_ have done so in the absence of incentives. As the results on the non-existent to small difference between play- and real-money markets shows us [@Mchugh2012; @ServanSchreiber2004], people potentially join prediction markets for a variety of reasons. So, to investigate the relevant counterfactual, we would ideally want experimental evidence about the causal question of whether accurate participants are (more) attracted by the presence of (accuracy) incentives, compared to no incentives. 

That, of course, is exactly the type of experiment we have reported on in this paper. When recruiting for this study, we recruited for the incentivised conditions by noting the additional, accuracy dependent incentive (i.e., a maximum of £20, on top of the £0.38 received simply for participating). For the control condition, the recruitment text said no such thing; it simply mentioned the fixed money paid for participating. (See Section \ref{recruitment-texts} in the Appendix for the full recruitment texts.) This, in effect, provides a between-subjects test of the hypothesis under consideration that, if accurate respondents turn up in greater number in the presence of incentives, then we should expect to see more of these --- and, as a result, greater accuracy --- in the incentivised condition. But again, the evidence provided for PRACTICAL EQUIVALENCY suggests that this did not happen.

# Discussion

The results of this study call into question the idea that prediction markets tend to generate accurate outputs on account of incentivising accuracy, whether by motivating people to provide more honest estimates, collect further information, or --- as discussed in the previous section --- attracting accurate traders to at all join the market. As noted in \ref{experimental-conditions-and-incentives}, participants took significantly longer to complete the survey in both incentivised conditions, compared to the control group, suggesting greater effort and potentially also attempts on the part of participants to locate relevant information (at least in relation to the vaccination task). However, in neither condition was the level of accuracy significantly higher than in the control condition --- on the contrary, they were practically equivalent. This offers evidence that incentivising for accuracy does not make a practical difference to estimation errors in a set of estimation tasks mirroring the type of incentive structures typically found on such markets. 

This naturally raises the question of how we then are to explain the accuracy of such markets, if not with reference to incentives. One possible explanation is that prediction markets are best understood as a special case of what is sometimes referred to as expectation polls: polls or surveys asking people about participants' expectations in general (e.g., "Who will win the election?") [@murr2021; @RothschildWolfers2011] or in relation to their social circles specifically [@ahlstromvij-socialcircle; @Galesic:2018aa] as, opposed to about participants' intentions (e.g., "How would you vote, if there were an election today?"). Such expectation polls tend to outperform traditional intention aggregation polls, likely because the former tap into people's non-trivial amounts of knowledge about the intentions and preferences of others [@NisbettKunda1985], who are thereby implicitly sampled as well [@ahlstromvij-socialcircle]. 

Tapping into such implicit  samples has two benefits. First, it has the potential of mitigating the type of respondent selection issues that typically affect surveys [@weisberg2018], including non-response and coverage bias [@ahlstromvij-socialcircle; @Galesic:2018aa]. That is, even if some particular group of people is systematically not sampled or generally non-responsive if sampled, there is a non-zero probability that someone sampled has information about that group that thereby gets factored into their reported expectations. Second, rather than being forced to make one big inference about a preference distribution of a population on the basis of sampled preferences, as on the traditional opinion poll paradigm, someone conducting an expectation poll relies on a large number of inferences made by individual respondents, the errors of which will then cancel out if randomly distributed.

If this explanation is correct, the accuracy of prediction markets has nothing to with accuracy incentives, and is at least in part due to their asking the right type of question: a predictive question about respondents' expectations rather than about their own intentions or preferences. The qualifier 'in part' is important, since the present study does not speak to the question of the relative merits of simply surveying expectations and aggregating by way of some summary statistic such as the mean, as in this study, as opposed to aggregating those expectations through a more sophisticated prediction market mechanism, whereby each estimate dynamically moves the price signal (interpreted as the group's aggregate estimate) over time. This, however, is a question that has already been addressed by others. For example, in a sample of 535 participants and 113 forecasting questions, @tetlock2019 found that aggregating using an prediction market mechanism outperformed a simple mean approach, in terms of minimizing estimate error. This suggests that levels of accuracy are likely underestimated in the present study, and that there is an added accuracy benefit from aggregating specifically through a market mechanism, over an above any benefit derived from opting for an expectation aggregation approach over a traditional intention aggregation approach.

# References

<div id="refs"></div>

# Appendix

## Data
The data and code underlying the present paper can be accessed at [removed for blind review].

## Participants

```{r participant_table, echo=FALSE, message=FALSE}
sample_gender <- table(all_data$gender,all_data$condition)
sample_age <- table(all_data$age_cat,all_data$condition)
sample_education <- table(all_data$education,all_data$condition)

sample_df <- data.frame(rbind(sample_gender,
                              sample_age,
                              sample_education,
                              c(length(all_data$prolific_id[all_data$condition=="external"]),
                                length(all_data$prolific_id[all_data$condition=="no_res"]),
                                length(all_data$prolific_id[all_data$condition=="self_res"]))))

row.names(sample_df) <- c("Female",
                          "Male",
                          "Gender not disclosed",
                          "Age: 18-24",
                          "Age: 25-34",
                          "Age: 35-44",
                          "Age: 45-54",
                          "Age: 55-64",
                          "Age: Over 65",
                          "Age not disclosed",
                          "Education: A-level",
                          "Education: GCSE",
                          "Education: None",
                          "Education: Postgraduate",
                          "Education: Undergraduate",
                          "N")

sample_df <- sample_df[, c(2,1,3)]
sample_df <- sample_df[c(1:10,12,11,15,14,13,16),]
#colnames(sample_df) <- c("External resolution","Self-resolution","Control")

knitr::kable(sample_df,caption="Number of participants per condition by category",format="latex",
             booktabs=TRUE, escape = FALSE, longtable = TRUE, linesep = "", label="participant_table", 
             col.names = linebreak(c("Control",
                                     "External resolution",
                                     "Self-resolution"), align = "c")) %>%
  kable_styling(latex_options = c("hold_position", "repeat_header"))
```

## Recruitment texts

Participants for the external and self-resolution condition, were recruited via the following text:

> _In this study, we will ask you two questions that will involve making an estimate. We will also ask you about your level of education._  

> _We expect this to take about 3 minutes, and will pay you £0.38 (corresponding to £7.60 per hour) for participating._  

> _Additionally, everyone giving the correct answer will take part in a lottery, where one person will be awarded £10, on top of the incentive received for participating._  

> _This means that, if you answer both questions correctly, you stand a chance to win £20, on top of the £0.38 you receive for simply participating._  

> _Participation is voluntary. You may withdraw at any point, and ask to have any data associated with your participation deleted by contacting the lead researcher, [removed for blind review]. If you withdraw, you forfeit your incentive payment._

Participants for the control condition, were recruited via the following text:

> _In this study, we will ask you two questions that will involve making an estimate. We will also ask you about your level of education._

> _We expect this to take about 3 minutes, and will pay you £0.38 (corresponding to £7.60 per hour) for participating._

> _Participation is voluntary. You may withdraw at any point, and ask to have any data associated with your participation deleted by contacting the lead researcher, [removed for blind review]. If you withdraw, you forfeit your incentive payment._